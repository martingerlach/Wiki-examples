{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://stat1005.eqiad.wmnet:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Pyspark notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f5550cf5d30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "import datetime\n",
    "import calendar\n",
    "import time\n",
    "import string\n",
    "import random\n",
    "\n",
    "import findspark\n",
    "findspark.init('/usr/lib/spark2')\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F, types as T, Window\n",
    "import wmfdata.spark as wmfspark\n",
    "\n",
    "## defining the spark session\n",
    "spark_config = {}\n",
    "# regular\n",
    "spark_config = {\n",
    "    \"spark.driver.memory\": \"2g\",\n",
    "    \"spark.dynamicAllocation.maxExecutors\": 64,\n",
    "    \"spark.executor.memory\": \"8g\",\n",
    "    \"spark.executor.cores\": 4,\n",
    "    \"spark.sql.shuffle.partitions\": 256\n",
    "}\n",
    "# ## big\n",
    "# spark_config = {\n",
    "#     \"spark.driver.memory\": \"4g\",\n",
    "#     \"spark.dynamicAllocation.maxExecutors\": 128,\n",
    "#     \"spark.executor.memory\": \"8g\",\n",
    "#     \"spark.executor.cores\": 4,\n",
    "#     \"spark.sql.shuffle.partitions\": 512,\n",
    "#     \"spark.sql.broadcastTimeout\": 3600\n",
    "# }\n",
    "# spark_config = {\n",
    "#     \"spark.dynamicAllocation.maxExecutors\": 128,\n",
    "#     \"spark.executor.memory\": \"16g\",\n",
    "#     \"spark.driver.memory\": \"12g\",\n",
    "#     \"spark.executor.memoryOverhead\":\"4g\",\n",
    "#     \"spark.num.executors\":50,\n",
    "#     \"spark.driver.maxResultSize\":\"32g\",\n",
    "# }\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'notebook'\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3.7'\n",
    "\n",
    "spark = wmfspark.get_session(\n",
    "    app_name='Pyspark notebook', \n",
    "    extra_settings=spark_config\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define wiki and snapshot for analysis\n",
    "wiki = 'simplewiki'\n",
    "wiki = 'enwiki'\n",
    "snapshot = '2020-07'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## all pages in the main namespace (incl redirects)\n",
    "# page_id, page_title, page_is_redirect\n",
    "df_pages = (\n",
    "    ## select table\n",
    "    spark.read.table('wmf_raw.mediawiki_page')\n",
    "    ## select wiki project\n",
    "    .where( F.col('wiki_db') == wiki )\n",
    "    .where( F.col('snapshot') == snapshot )\n",
    "    ## main namespace\n",
    "    .where(F.col('page_namespace') == 0 )\n",
    "    .select(\n",
    "        'page_id',\n",
    "        'page_title',\n",
    "        'page_is_redirect'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## redirects table with page_ids from/to\n",
    "## we join the pages table to get page_id for the redirected-to page\n",
    "df_redirect = (\n",
    "    ## select table\n",
    "    spark.read.table('wmf_raw.mediawiki_redirect')\n",
    "    ## select wiki project\n",
    "    .where( F.col('wiki_db') == wiki )\n",
    "    .where( F.col('snapshot') == snapshot )\n",
    "    .where(F.col('rd_namespace') == 0 )\n",
    "    .select(\n",
    "        F.col('rd_from').alias('page_id_from'),\n",
    "        F.col('rd_title').alias('page_title')\n",
    "    )\n",
    "    \n",
    "    ## get the page-ids for the redirected-to pages\n",
    "    .join(df_pages,on='page_title',how='inner')\n",
    "    \n",
    "    ## select only page-ids\n",
    "    .select(\n",
    "        F.col('page_id_from').alias('rd_from'),\n",
    "        F.col('page_id').alias('rd_to')\n",
    "    )\n",
    ")\n",
    "# df_redirect.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the pagelinks table with page_id_from and page_id_to\n",
    "## only keep links starting from non-redirect pages\n",
    "## join pages-table to get page-ids for redirect-to pages\n",
    "df_links = (\n",
    "    ## select table\n",
    "    spark.read.table('wmf_raw.mediawiki_pagelinks')\n",
    "    ## select wiki project\n",
    "    .where( F.col('wiki_db') == wiki )\n",
    "    .where( F.col('snapshot') == snapshot )\n",
    "    \n",
    "    ## namespace of source and target page\n",
    "    .where(F.col('pl_from_namespace') == 0 )\n",
    "    .where(F.col('pl_namespace') == 0 )\n",
    "    \n",
    "    .withColumnRenamed('pl_from','page_id_from')\n",
    "    .withColumnRenamed('pl_title','page_title')\n",
    "    \n",
    "    ## only keep links that originate from a page that is not a redirect \n",
    "    ## by joining the pages-table with the non-redirect pages\n",
    "    .join(\n",
    "        df_redirect.withColumnRenamed('rd_from','page_id_from'),\n",
    "        on = 'page_id_from',\n",
    "        how = 'left_anti'\n",
    "    )\n",
    "    ## map page_title_to page_id_to by joining the pages-df\n",
    "    .join(\n",
    "        df_pages,\n",
    "        on='page_title',\n",
    "        how='inner'\n",
    "    )\n",
    "    .withColumnRenamed('page_id','page_id_to')\n",
    "    .select('page_id_from','page_id_to')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## resolve the redirects in the links-table by joining the redirect table\n",
    "df_links_resolved = (\n",
    "    df_links\n",
    "    ## join in the redirects\n",
    "    .join(\n",
    "        df_redirect,\n",
    "        df_links['page_id_to'] == df_redirect['rd_from'],\n",
    "        how = 'left'\n",
    "    )\n",
    "    ## select the redirected link (otherwise keep the old)\n",
    "    .withColumn('page_id_to_resolved', F.coalesce(F.col('rd_to'),F.col('page_id_to')) )\n",
    "    .select(\n",
    "        F.col('page_id_from').alias('page_id_from'),\n",
    "        F.col('page_id_to_resolved').alias('page_id_to')\n",
    "    )\n",
    "    ## remove duplicate links\n",
    "    .distinct()\n",
    "    .select(\n",
    "        'page_id_from',\n",
    "        'page_id_to'\n",
    "    )\n",
    "#     .orderBy('page_id_from','page_id_to')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILE_out = '/user/mgerlach/graph/test_%s_%s'%(wiki,snapshot)\n",
    "# df_links_resolved.write.mode('overwrite').csv(path=FILE_out, compression=\"gzip\", header=True, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## converting to a local file\n",
    "base_dir_hdfs = '/user/mgerlach/graph'\n",
    "PATH_OUT = '/home/mgerlach/REPOS/reader-embedding/output/graph/'\n",
    "# filename_save = 'graph_%s_%s_nodes'%(wiki,snapshot)\n",
    "filename_save = 'test_%s_%s_links.csv.gz'%(wiki,snapshot)\n",
    "output_hdfs_dir = os.path.join(base_dir_hdfs,filename_save)\n",
    "# os.system('hadoop fs -rm -r %s'%output_hdfs_dir)\n",
    "df_links_resolved.write.mode('overwrite').csv(path=output_hdfs_dir, compression=\"gzip\", header=False, sep=\"\\t\")\n",
    "base_dir_local =  PATH_OUT\n",
    "output_local_dir_tmp = os.path.join(base_dir_local,'tmp',filename_save)\n",
    "output_local_file = os.path.join(base_dir_local,filename_save)\n",
    "\n",
    "\n",
    "os.system('hadoop fs -copyToLocal %s %s'%(output_hdfs_dir,output_local_dir_tmp))\n",
    "## concatenate and unzip into single file\n",
    "# os.system('cat %s/* | gunzip > %s'%(output_local_dir_tmp,output_local_file))\n",
    "os.system('cat %s/* > %s'%(output_local_dir_tmp,output_local_file))\n",
    "## remove set of tmp-dirs\n",
    "os.system('rm -rf %s'%output_local_dir_tmp)\n",
    "## remove hadoop data\n",
    "os.system('hadoop fs -rm -r %s'%output_hdfs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## join the wikidata-item to each pageview\n",
    "## we keep only pageviews for which we have a correpsionding wikidata-item id\n",
    "\n",
    "## table with mapping wikidata-ids to page-ids\n",
    "## partition wikidb and page-id ordered by snapshot\n",
    "w_wd = Window.partitionBy(F.col('wiki_db'),F.col('page_id')).orderBy(F.col('snapshot').desc())\n",
    "df_wd = (\n",
    "    spark.read.table('wmf.wikidata_item_page_link')\n",
    "    ## snapshot: this is a partition!\n",
    "    .where(F.col('snapshot') >= '2020-07-01') ## resolve issues with non-mathcing wikidata-items\n",
    "    ## only wikis (enwiki, ... not: wikisource)\n",
    "    .where(F.col('wiki_db')==wiki)\n",
    "    .withColumn('item_id_latest',F.first(F.col('item_id')).over(w_wd))\n",
    "    .select(\n",
    "        'page_id',\n",
    "        F.col('item_id_latest').alias('item_id')\n",
    "    )\n",
    "    .drop_duplicates()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from = df_links_resolved.select('page_id_from').distinct().withColumnRenamed('page_id_from','page_id')\n",
    "df_to = df_links_resolved.select('page_id_to').distinct().withColumnRenamed('page_id_to','page_id')\n",
    "df_nodes_sel = df_from.join(df_to,on='page_id',how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all nodes from the pages-table which appear in the links_resolved-table (from/to)\n",
    "df_nodes = (\n",
    "    df_pages\n",
    "    .join(\n",
    "        df_nodes_sel,\n",
    "        on = 'page_id',\n",
    "        how = 'left_semi'\n",
    "    )\n",
    "    .join(df_wd,on='page_id',how='left')\n",
    "    .select(\n",
    "        'page_id',\n",
    "        'page_title',\n",
    "        'item_id'\n",
    "    )\n",
    ")\n",
    "\n",
    "base_dir_hdfs = '/user/mgerlach/graph'\n",
    "PATH_OUT = '/home/mgerlach/REPOS/reader-embedding/output/graph/'\n",
    "filename_save = 'test_%s_%s_nodes.csv.gz'%(wiki,snapshot)\n",
    "output_hdfs_dir = os.path.join(base_dir_hdfs,filename_save)\n",
    "df_nodes.write.mode('overwrite').csv(path=output_hdfs_dir, compression=\"gzip\", header=False, sep=\"\\t\")\n",
    "base_dir_local =  PATH_OUT\n",
    "output_local_dir_tmp = os.path.join(base_dir_local,'tmp',filename_save)\n",
    "output_local_file = os.path.join(base_dir_local,filename_save)\n",
    "os.system('hadoop fs -copyToLocal %s %s'%(output_hdfs_dir,output_local_dir_tmp))\n",
    "## concatenate and unzip into single file\n",
    "# os.system('cat %s/* | gunzip > %s'%(output_local_dir_tmp,output_local_file))\n",
    "os.system('cat %s/*  > %s'%(output_local_dir_tmp,output_local_file))\n",
    "## remove set of tmp-dirs\n",
    "os.system('rm -rf %s'%output_local_dir_tmp)\n",
    "## remove hadoop data\n",
    "os.system('hadoop fs -rm -r %s'%output_hdfs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_wmfdata",
   "language": "python",
   "name": "venv_wmfdata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
