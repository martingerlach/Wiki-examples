{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mwparserfromhell on spark\n",
    "\n",
    "Short description on how to use mwparserfromhell with spark; for example when trying to parse the [wikitext-dump](https://wikitech.wikimedia.org/wiki/Analytics/Data_Lake/Content/Mediawiki_wikitext_current).\n",
    "\n",
    "\n",
    "This currently only works on stat1008.\n",
    "\n",
    "The mwarserfromhell is installed on the workers of the cluster as part of the [conda base-environment](https://wikitech.wikimedia.org/wiki/Analytics/Systems/Anaconda). Thus you only have to make sure that the kernel of the notebook uses that environment. I managed to set this up in the following way:\n",
    "\n",
    "- loging into stat1008: `ssh stat1008.eqiad.wmnet`\n",
    "\n",
    "- activate the base environment: `source /usr/lib/anaconda-wmf/bin/activate`\n",
    "\n",
    "- add the environment to jupyter-kernels: `ipython kernel install --user --name=venv_anaconda-wmf` (you can give it a different name than venv_anaconda-wmf)\n",
    "\n",
    "- start jupyterhub: `ssh -N stat1008.eqiad.wmnet -L 8880:127.0.0.1:8880` and type `http://localhost:8880/` in browser\n",
    "\n",
    "- you should now see a kernel `venv_anaconda-wmf` to run your notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://stat1008.eqiad.wmnet:4044\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Pyspark notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f84b528b0d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "import datetime\n",
    "import calendar\n",
    "import time\n",
    "import string\n",
    "import random\n",
    "\n",
    "import findspark\n",
    "findspark.init('/usr/lib/spark2')\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F, types as T, Window\n",
    "import wmfdata.spark as wmfspark\n",
    "\n",
    "## defining the spark session\n",
    "spark_config = {}\n",
    "spark = wmfspark.get_session(\n",
    "    app_name='Pyspark notebook', \n",
    "    type='regular'\n",
    "#     extra_settings=spark_config\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = 'simplewiki'\n",
    "lang = wiki.replace('wiki','')\n",
    "snapshot='2020-07'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+\n",
      "|   pid|               title|            wikitext|\n",
      "+------+--------------------+--------------------+\n",
      "|606975|Somerset MRT station|'''Somerset MRT S...|\n",
      "|  9523|       Welfare state|A '''welfare stat...|\n",
      "|765801|   Audrey Wasilewski|{{refimprove|date...|\n",
      "|463793|Star Trek Into Da...|'''Star Trek Into...|\n",
      "|638276|       Antaeus Group|{{Infobox company...|\n",
      "|301275|      Steve Montador|{{Infobox ice hoc...|\n",
      "| 18907|Providence, Rhode...|{{Infobox settlem...|\n",
      "|660137|  Knox County, Texas|'''Knox County'''...|\n",
      "|455365|       Shashi Kapoor|{{Infobox person\n",
      "...|\n",
      "|498956|     Diahann Carroll|[[Image:Diahannca...|\n",
      "|138161|Roquefort, Lot-et...|{{Infobox French ...|\n",
      "|746807|          Serena Liu|{{Chinese name|[[...|\n",
      "|656456|       Waucoma, Iowa|'''Waucoma''' is ...|\n",
      "|409789|Penelope (given n...|'''Penelope''' is...|\n",
      "|151772|              Angaïs|'''Angaïs''' is a...|\n",
      "|441945|     Chandragupta II|{{no sources|date...|\n",
      "| 60415|      George Adamski|'''George Adamski...|\n",
      "|713649|           Tall Girl|{{Infobox film\n",
      "| ...|\n",
      "|631727|      Grashof number|{{no footnotes|da...|\n",
      "|377954|       Gunner's mate|The [[United Stat...|\n",
      "+------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Get some articles from the wikitext-dump\n",
    "## only articles (no redirect title)\n",
    "articles = (\n",
    "    ## select table\n",
    "    spark.read.table('wmf.mediawiki_wikitext_current')\n",
    "    ## select wiki project\n",
    "    .where( F.col('wiki_db') == wiki )\n",
    "    .where( F.col('snapshot') == snapshot )\n",
    "    ## main namespace\n",
    "    .where(F.col('page_namespace') == 0 )\n",
    "    ## no redirect-pages\n",
    "    .where(F.col('page_redirect_title')=='')\n",
    "    .where(F.col('revision_text').isNotNull())\n",
    "    .where(F.length(F.col('revision_text'))>0)\n",
    "    .select(\n",
    "        F.col('page_id').alias('pid'),\n",
    "        F.col('page_title').alias('title'),\n",
    "        F.col('revision_text').alias('wikitext')\n",
    "    )\n",
    "    .limit(100)\n",
    ")\n",
    "articles.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mwparserfromhell\n",
    "import urllib\n",
    "import re\n",
    "links_regex = re.compile(r\"\\[\\[(?P<link>[^\\n\\|\\]\\[\\<\\>\\{\\}]{0,256})(?:\\|(?P<anchor>[^\\[]*?))?\\]\\]\")\n",
    "references_regex = re.compile(r\"<ref[^>]*>[^<]+<\\/ref>\")\n",
    "def get_plain_text_without_links(row):\n",
    "    \"\"\" Replace the links with a dot to interrupt the sentence and get the plain text \"\"\"\n",
    "    wikicode = row.wikitext\n",
    "    wikicode_without_links = re.sub(links_regex, '.', wikicode)\n",
    "    wikicode_without_links = re.sub(references_regex, '.', wikicode_without_links)\n",
    "    ## we dont have mwparserfromhell on the spark-cluster yet\n",
    "    try:\n",
    "        text = mwparserfromhell.parse(wikicode_without_links).strip_code()\n",
    "    except:\n",
    "        text = wikicode_without_links\n",
    "    return T.Row(pid=row.pid, title=normalise_title(row.title), text=text.lower())\n",
    "def normalise_title(title):\n",
    "    \"\"\" Replace _ with space, remove anchor, capitalize \"\"\"\n",
    "    title = urllib.parse.unquote(title)\n",
    "    title = title.strip()\n",
    "    if len(title) > 0:\n",
    "        title = title[0].upper() + title[1:]\n",
    "    n_title = title.replace(\"_\", \" \")\n",
    "    if '#' in n_title:\n",
    "        n_title = n_title.split('#')[0]\n",
    "    return n_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>502075</td>\n",
       "      <td>matthew carle (born september 25, 1984) is an ...</td>\n",
       "      <td>Matt Carle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63261</td>\n",
       "      <td>zanzibar is the name of an . in the . 25–50 km...</td>\n",
       "      <td>Zanzibar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>215017</td>\n",
       "      <td>.\\nthe quebec nordiques (, pronounced  in .,  ...</td>\n",
       "      <td>Quebec Nordiques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>534095</td>\n",
       "      <td>.\\naubrey kerr mcclendon (july 14, 1959 – marc...</td>\n",
       "      <td>Aubrey McClendon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>469807</td>\n",
       "      <td>glen albert larson (january 3, 1937 - november...</td>\n",
       "      <td>Glen A. Larson</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pid                                               text             title\n",
       "0  502075  matthew carle (born september 25, 1984) is an ...        Matt Carle\n",
       "1   63261  zanzibar is the name of an . in the . 25–50 km...          Zanzibar\n",
       "2  215017  .\\nthe quebec nordiques (, pronounced  in .,  ...  Quebec Nordiques\n",
       "3  534095  .\\naubrey kerr mcclendon (july 14, 1959 – marc...  Aubrey McClendon\n",
       "4  469807  glen albert larson (january 3, 1937 - november...    Glen A. Larson"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_plain_text =  (\n",
    "    spark.createDataFrame(\n",
    "        articles\n",
    "        .rdd\n",
    "        .map(get_plain_text_without_links)\n",
    "    )\n",
    ")\n",
    "df = articles_plain_text.toPandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"matthew carle (born september 25, 1984) is an . professional . .. he currently plays for the . of the . (nhl). he has also played for the . and ..\\n\\ncareer\\nbefore playing in the nhl, carle played parts of 2 seasons with the ., 1 season with the . of the . (ushl), and 3 years of college hockey with the . pioneers. during his time with the pioneers, carle won the . in 2006 for being the top . men's ice hockey player. he was also the only junior defenseman in history to win the award..\\n\\nhe was drafted 47th overall by the . in the .. on march 25, 2006, carle made his nhl debut and scored his first nhl goal against . of the . in a 5-1 win.. on november 21, 2007, carle was signed to a four-year, $13.75 million contract extension to stay with san jose..\\n\\non july 4, 2008, the sharks traded carle along with . and a first round pick in the 2009 nhl entry draft and a fourth round pick in 2010, to the . in exchange for . and ...\\n\\nafter 12 games with the lightning, they traded him along with a 2009 third round draft pick to the . in exchange for ., . and a 2009 fourth round pick on november 7, 2008.. carle missed 5 games at the start of december because of a rib injury. during the home opener of the . for the flyers, carle tied the nhl record for assists in a single period by a defenceman when he set up four goals in the second period against the . and also helped . achieve his second career hat trick, which was also his personal best for assists in a game..\\n\\non july 4, 2012, carle signed a six-year; $33 million contract to return with the tampa bay lightning..\\n\\nreferences\\n\\n other websites \\n\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_anaconda",
   "language": "python",
   "name": "venv_anaconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
