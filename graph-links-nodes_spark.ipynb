{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the full graph from a wiki\n",
    "\n",
    "We want to get all links between pages in a wikipedia.\n",
    "\n",
    "The challenges are:\n",
    "- the pagelinks table contains the page-id for the source-page and the page-title for the target page; thus we have to join the page-table to get also pageids for the target-pages\n",
    "- the pagelinks table contains links to redirect pages; we resolve redirects by joining the redirect-table\n",
    "- we want to add the wikidata-item for each page; for this we merge the wikidata_item_page_link-table\n",
    "\n",
    "The output are 2 tables in /user/mgerlach/graph/\n",
    "- graph-<wiki>-<snapshot>_links.parquet\n",
    "    - adjacency list in the form: page_id_from page_id_to\n",
    "    \n",
    "- graph-<wiki>-<snapshot>_nodes.parquet\n",
    "    - list of all nodes contained in the adjacency list in the form: page_id page_title item_id\n",
    "\n",
    "- with parameters:\n",
    "    - <wiki> = the wikipedia, e.g. enwiki\n",
    "    - <snapshot> = time-stamp of snapshot, e.g. 2020-07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://stat1005.eqiad.wmnet:4045\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Pyspark notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f7d79076978>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "import datetime\n",
    "import calendar\n",
    "import time\n",
    "import string\n",
    "import random\n",
    "\n",
    "import findspark\n",
    "findspark.init('/usr/lib/spark2')\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F, types as T, Window\n",
    "import wmfdata.spark as wmfspark\n",
    "\n",
    "## defining the spark session\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'notebook'\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3.7'\n",
    "\n",
    "spark = wmfspark.get_session(\n",
    "    app_name='Pyspark notebook', \n",
    "    type='large'\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define wiki and snapshot for analysis\n",
    "# wiki = 'simplewiki'\n",
    "wiki = 'enwiki'\n",
    "snapshot = '2020-07'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table with all links\n",
    "\n",
    "- only namespace 0\n",
    "- resolve redirects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## all pages in the main namespace (incl redirects)\n",
    "# page_id, page_title, page_is_redirect\n",
    "df_pages = (\n",
    "    ## select table\n",
    "    spark.read.table('wmf_raw.mediawiki_page')\n",
    "    ## select wiki project\n",
    "    .where( F.col('wiki_db') == wiki )\n",
    "    .where( F.col('snapshot') == snapshot )\n",
    "    ## main namespace\n",
    "    .where(F.col('page_namespace') == 0 )\n",
    "    .select(\n",
    "        'page_id',\n",
    "        'page_title',\n",
    "        'page_is_redirect'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## redirects table with page_ids from/to\n",
    "## we join the pages table to get page_id for the redirected-to page\n",
    "df_redirect = (\n",
    "    ## select table\n",
    "    spark.read.table('wmf_raw.mediawiki_redirect')\n",
    "    ## select wiki project\n",
    "    .where( F.col('wiki_db') == wiki )\n",
    "    .where( F.col('snapshot') == snapshot )\n",
    "    .where(F.col('rd_namespace') == 0 )\n",
    "    .select(\n",
    "        F.col('rd_from').alias('page_id_from'),\n",
    "        F.col('rd_title').alias('page_title')\n",
    "    )\n",
    "    \n",
    "    ## get the page-ids for the redirected-to pages\n",
    "    .join(df_pages,on='page_title',how='inner')\n",
    "    \n",
    "    ## select only page-ids\n",
    "    .select(\n",
    "        F.col('page_id_from').alias('rd_from'),\n",
    "        F.col('page_id').alias('rd_to')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the pagelinks table with page_id_from and page_id_to\n",
    "## only keep links starting from non-redirect pages\n",
    "## join pages-table to get page-ids for redirect-to pages\n",
    "df_links = (\n",
    "    ## select table\n",
    "    spark.read.table('wmf_raw.mediawiki_pagelinks')\n",
    "    ## select wiki project\n",
    "    .where( F.col('wiki_db') == wiki )\n",
    "    .where( F.col('snapshot') == snapshot )\n",
    "    \n",
    "    ## namespace of source and target page\n",
    "    .where(F.col('pl_from_namespace') == 0 )\n",
    "    .where(F.col('pl_namespace') == 0 )\n",
    "    \n",
    "    .withColumnRenamed('pl_from','page_id_from')\n",
    "    .withColumnRenamed('pl_title','page_title')\n",
    "    \n",
    "    ## only keep links that originate from a page that is not a redirect \n",
    "    ## by joining the pages-table with the non-redirect pages\n",
    "    .join(\n",
    "        df_redirect.withColumnRenamed('rd_from','page_id_from'),\n",
    "        on = 'page_id_from',\n",
    "        how = 'left_anti'\n",
    "    )\n",
    "    ## map page_title_to page_id_to by joining the pages-df\n",
    "    .join(\n",
    "        df_pages,\n",
    "        on='page_title',\n",
    "        how='inner'\n",
    "    )\n",
    "    .withColumnRenamed('page_id','page_id_to')\n",
    "    .select('page_id_from','page_id_to')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## resolve the redirects in the links-table by joining the redirect table\n",
    "df_links_resolved = (\n",
    "    df_links\n",
    "    ## join in the redirects\n",
    "    .join(\n",
    "        df_redirect,\n",
    "        df_links['page_id_to'] == df_redirect['rd_from'],\n",
    "        how = 'left'\n",
    "    )\n",
    "    ## select the redirected link (otherwise keep the old)\n",
    "    .withColumn('page_id_to_resolved', F.coalesce(F.col('rd_to'),F.col('page_id_to')) )\n",
    "    .select(\n",
    "        F.col('page_id_from').alias('page_id_from'),\n",
    "        F.col('page_id_to_resolved').alias('page_id_to')\n",
    "    )\n",
    "    ## remove duplicate links\n",
    "    .distinct()\n",
    "    .select(\n",
    "        'page_id_from',\n",
    "        'page_id_to'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_out = '/user/mgerlach/graph/graph-%s-%s_links.parquet'%(wiki,snapshot)\n",
    "df_links_resolved.write.mode('overwrite').parquet(FILE_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## table with all nodes\n",
    "- pid\n",
    "- page_title\n",
    "- merge qid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## join the wikidata-item to each pageview\n",
    "## we keep only pageviews for which we have a correpsionding wikidata-item id\n",
    "\n",
    "## table with mapping wikidata-ids to page-ids\n",
    "## partition wikidb and page-id ordered by snapshot\n",
    "w_wd = Window.partitionBy(F.col('wiki_db'),F.col('page_id')).orderBy(F.col('snapshot').desc())\n",
    "df_wd = (\n",
    "    spark.read.table('wmf.wikidata_item_page_link')\n",
    "    ## snapshot: this is a partition!\n",
    "    .where(F.col('snapshot') >= '2020-07-01') ## resolve issues with non-mathcing wikidata-items\n",
    "    ## only wikis (enwiki, ... not: wikisource)\n",
    "    .where(F.col('wiki_db')==wiki)\n",
    "    .withColumn('item_id_latest',F.first(F.col('item_id')).over(w_wd))\n",
    "    .select(\n",
    "        'page_id',\n",
    "        F.col('item_id_latest').alias('item_id')\n",
    "    )\n",
    "    .drop_duplicates()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from = df_links_resolved.select('page_id_from').distinct().withColumnRenamed('page_id_from','page_id')\n",
    "df_to = df_links_resolved.select('page_id_to').distinct().withColumnRenamed('page_id_to','page_id')\n",
    "df_nodes_sel = df_from.join(df_to,on='page_id',how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all nodes from the pages-table which appear in the links_resolved-table (from/to)\n",
    "df_nodes = (\n",
    "    df_pages\n",
    "    .join(\n",
    "        df_nodes_sel,\n",
    "        on = 'page_id',\n",
    "        how = 'left_semi'\n",
    "    )\n",
    "    .join(df_wd,on='page_id',how='left')\n",
    "    .select(\n",
    "        'page_id',\n",
    "        'page_title',\n",
    "        'item_id'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_out = '/user/mgerlach/graph/graph-%s-%s_nodes.parquet'%(wiki,snapshot)\n",
    "df_nodes.write.mode('overwrite').parquet(FILE_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### links file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_in = '/user/mgerlach/graph/graph-%s-%s_links.parquet'%(wiki,snapshot)\n",
    "df_check = spark.read.load(FILE_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+\n",
      "|page_id_from|page_id_to|\n",
      "+------------+----------+\n",
      "|    18914017|      2088|\n",
      "|    13907534|      6824|\n",
      "+------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_check.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "514160225"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_check.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nodes file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_in = '/user/mgerlach/graph/graph-%s-%s_nodes.parquet'%(wiki,snapshot)\n",
    "df_check = spark.read.load(FILE_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+---------+\n",
      "|page_id|          page_title|  item_id|\n",
      "+-------+--------------------+---------+\n",
      "|   3234|Archbishopric_of_...|Q15471939|\n",
      "|   5390| Conversion_of_units|  Q618655|\n",
      "+-------+--------------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_check.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6151649"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_check.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_wmfdata",
   "language": "python",
   "name": "venv_wmfdata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
